==================================================
üßæ FDA SUBMISSION PACKAGE ‚Äì SIMULATED
Software as a Medical Device (SaMD)
Title: PERG Signal Classifier via ML & DL
Author: Anis Boubala
Date: July 2025
==================================================

1. DEVICE OVERVIEW
-------------------
**Device Name**: PERG Signal Classifier  
**Type**: Software as a Medical Device (SaMD)  
**Indication for Use**:  
> To assist ophthalmologists in identifying alterations in pattern electroretinogram (PERG) signals that may be associated with early-stage retinal or optic nerve dysfunction (e.g., glaucoma).

**Device Description**:  
A software-based tool for classifying PERG electrophysiological signals (RE_1, LE_1) using:
- Classical Machine Learning (ML) trained on FFT + clinical features
- Deep Learning (DL) on raw signal concatenation

**Output**: Binary prediction ‚Äî `"Normal PERG"` or `"Altered PERG"`

---

2. INTENDED USE
-------------------
- **Patient Population**: Adults aged 18‚Äì90 undergoing PERG testing  
- **Use Environment**: Ophthalmology clinics, screening centers  
- **Users**: Ophthalmologists or trained technicians  
- **Device Role**: Clinical decision support (not a standalone diagnostic tool)  
- **Limitations**: No FDA clearance. Requires confirmation via clinical evaluation.

---

3. DATA & LABELING
-------------------
**Source**: PERG-IOBA Dataset (PhysioNet)  
**Size**: 279 labeled patient records  
**Signal Channels**: RE_1, LE_1 (microvolt resolution, 300 Hz)  
**Metadata**: Age, Sex, Diagnosis  
**Ground Truth**: Diagnosis from `metadata_clinical.csv` (Ophthalmologist-reviewed)

---

4. MODELS EVALUATED
--------------------
**Classical ML**:
- Logistic Regression  
- Decision Tree  
- Random Forest (Best ML performer)  
- XGBoost  

**Deep Learning**:
- Fully connected neural network (Keras, TensorFlow backend)  
- Input: Concatenated signal + metadata  
- Layers: Dense ‚Üí Dropout ‚Üí BatchNorm (3‚Äì4 layers)

**Best Threshold Strategy**:  
- Youden Index (Seuil optimal pour maximiser sensibilit√© + sp√©cificit√©)  
- F1-maximizing threshold (priorise la balance Recall/Precision)

---

5. PERFORMANCE SUMMARY
-----------------------
| Model Type | AUC     | Recall | Specificity | F1-Score | Accuracy | Threshold |
|------------|---------|--------|-------------|----------|----------|-----------|
| Random Forest (ML) | **0.768** | 0.70   | 0.70        | 0.69     | **0.71**   | 0.50 (default) |
| Deep Learning       | 0.66    | **0.93** | 0.33        | **0.76** | 0.67     | 0.45 (F1-opt)  |

üî¨ **Interpretation**:
- ML model (RF) yields a better *global AUC*, suitable for balanced screening.
- DL model offers **high sensitivity**, favoring early detection of pathological PERG but may yield false positives (lower specificity).

---

6. GOOD MACHINE LEARNING PRACTICES (GMLP)
------------------------------------------
‚úì Balanced training & real-world test splits  
‚úì Feature explainability for ML (FFT, stats, metadata)  
‚úì Deep learning transparency: open architecture and training logs  
‚úì ROC-based threshold optimization with validation report  
‚úì Version control (Git), reproducibility (random seeds, requirements.txt)  
‚úì Ground truth verified via structured clinical metadata

---

7. VALIDATION STRATEGY
------------------------
- Cross-validation: stratified 5-fold on training set  
- Separate test set:
  - **Balanced (50/50)**  
  - **Realistic prevalence (~2%)**  
- ROC, confusion matrices, precision-recall curves  
- Threshold optimization strategies logged and visualized

---

8. RISK ANALYSIS (Inspired by ISO 14971)
-----------------------------------------
| Risk                         | Mitigation                             |
|------------------------------|----------------------------------------|
| ‚ùå False Negative (missed disease) | Seuil optimis√© pour Recall √©lev√© (TPR ‚â• 0.90) |
| ‚ö†Ô∏è False Positive (unnecessary referral) | Review by clinician; specificity tracked |
| ‚ö†Ô∏è Dataset Bias              | Stratification + testing on prevalence-based split |
| ‚ùå Overfitting (small dataset) | Dropout, CV, early stopping, hyperparameter tuning |

---

9. LIMITATIONS
----------------
- No prospective external clinical validation  
- No EHR integration or device interoperability  
- Signal quality variability (not corrected for all noise/artifact cases)  
- Model trained on historical data ‚Äî no real-time acquisition  
- Ground truth limited to dataset-provided clinical tags

---

10. FUTURE WORK
----------------
- FDA pre-submission pathway (if productized)  
- External multicenter validation (clinic vs control)  
- Integration in a GUI or web-based diagnostic assistant  
- Real-time acquisition interface (e.g., PERG machine API)  
- Extension to time-series deep models (e.g., CNN, LSTM)

---

11. DISCLAIMER
---------------
This document simulates a pre-submission for FDA regulatory purposes.  
The tool is **not cleared or approved** by the FDA and is strictly for **academic demonstration** in bioinformatics and digital ophthalmology.


